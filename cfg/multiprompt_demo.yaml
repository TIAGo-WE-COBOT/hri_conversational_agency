model:
  model: "qwen3:4b"
  # See: https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html for a complete list of available parameters.
  temperature: 0.1
  max_tokens: 1000
  top_p: 0.95
  top_k: 40

router:
  prompt: |
    You are an assistant who has to choose which expert to route the question to.
    Experts are: 
    {experts}
    Based on the question, choose the expert that is most relevant to answer it. Respond with the name of the expert only.
    /no_think


#memory: True | False # enable or disable memory

incremental: False  # True | False 
                    # If True, the agent is updated keeping the same structure, and modifying the specified settings. If False, the agent is completely reloaded with the new settings.

chains:
  fallback:
    type: "chat"
    description: |
      answers on general questions, when no expert is found.
    prompt: |
      You are a helpful assistant. Here is the conversation so far: 
      {history}.
      Answer all questions to the best of your ability. 
      /no_think

  dummy:
    type: "tool"
    description: |
      answers questions about rainbows and unicorns.
    order: "random" # "sequential" | "random"
    responses:
      - "I love rainbows."
      - "I love unicorns."
    language: "en" # ["en", "it"]
  
  weather:
    type: "tool"
    description: |
      provides current weather information for the specified location.
    location: "New York"
    language: "en" # ["en", "it"]

  datetime:
    type: "tool"
    description: |
      provides current date, month, and season information.
    language: "en" # ["en", "it"]